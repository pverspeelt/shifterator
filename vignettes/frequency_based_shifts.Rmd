---
title: "Frequency-Based Shifts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{frequency_based_shifts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Many word shifts can be constructed using only the frequency counts of how often words appear in each of two texts. The frequencies should be loaded into two data.frames type2freq_1 and type2freq_2, where the first column contains the word and the second column contains the values that indicate how many times that word appeared in that text. For the rest of this vignette we will use the US presidential inaugural address texts from Ronald Reagan and George W. Bush. You can find these texts in the quanteda package.

```{r, message = FALSE}
library(shifterator)
library(quanteda)
library(quanteda.textstats)
library(dplyr)

reagan <- corpus_subset(data_corpus_inaugural, President == "Reagan") %>% 
  tokens(remove_punct = TRUE) %>% 
  dfm() %>% 
  textstat_frequency() %>% 
  as.data.frame() %>% # to move from classes frequency, textstat, and data.frame to data.frame
  select(feature, frequency) 

bush <- corpus_subset(data_corpus_inaugural, President == "Bush" & FirstName == "George W.") %>% 
  tokens(remove_punct = TRUE) %>% 
  dfm() %>% 
  textstat_frequency() %>% 
  as.data.frame() %>% 
  select(feature, frequency)
```

## Proportion Shifts
The easiest word shift graph that we can construct is a proportion shift. If $p_i^{(1)}$ is the relative frequency of word *i* in the first text, and $p_i^{(2)}$ is its relative frequency in the second text, then the proportion shift calculates their difference:

$$\delta p_i = p_i^{(2)} - p_i^{(1)}$$
If the difference is positive ($\delta p_i > 0$), then the word is relatively more common in the second text. If it is negative ($\delta p_i < 0$), then it is relatively more common in the first text. We can rank words by this difference and plot them as a word shift graph.

```{r, fig.align='center', fig.height=6, fig.width=7}
prop_shift <- proportion_shift(type2freq_1 = reagan, 
                               type2freq_2 = bush)
get_shift_graphs(prop_shift, text_names = c("Reagan", "Bush"))
```

We can see that in the plot above Reagan uses the words "government" and "people" more than Bush, while Bush uses the words "freedom" and "liberty" more than Reagan.

There are two diagnostic plots included on the right side of the main word shift plot. 

1. The first is the cumulative contribution plot, which traces how $\Sigma_i|\delta p_i|$
changes as we add more words according to their rank. The horizontal line shows cutoff of the word contributions that are plotted versus those that are not. This helps show how much of the overall difference between the texts is explained by the top contributing words. In this plot, about a 16 percent of the overall difference is explained by the top 50 words.
2. The second diagnostic plot shows the relative text size of each corpus, measured by the number of word tokens used. This plot tells us that the Reagan corpus is about 25 percent larger as the Bush corpus.

If we need the relative frequencies $p_i^{(1)}$, they can be accessed via the type2p_1 column in the shift_scores data.frame in the shift object and $p_i^{(2)}$ are in the type2p_1 column in the same data.frame. The differences $\delta p_i$ are in the type2shift_score column.

## Shannon Entropy Shifts
Proportion shifts are easy to interpret, but they are simplistic and have a difficult time pulling out interesting differences between two texts. For example, we see many “stop words” in the proportion shift. Instead, we can use the Shannon entropy to identify more “surprising” words and how they vary between two texts. The Shannon entropy *H* is calculated as:
$$H(P) = \sum_i p_i \log \frac{1}{p_i}$$
Where the factor $-\log p_i$ is the surprisal of a word. The less often a word appears in a text, the more surprising that it is. The Shannon entropy can be interpreted as the average surprisal of a text. We can compare two texts by taking the difference between their entropies, $H(P^{(2)}) - H(P^{(1)})$. When we do this, we can get the contribution $\delta H_i$ of each word to that difference:
$$\delta H_i = p_i^{(2)} \log \frac{1}{p_i^{(2)}} - p_i^{(1)} \log \frac{1}{p_i^{(1)}}$$
We can rank these contributions and plot them as a Shannon entropy word shift. If the contribution $\delta H_i$ is positive, then word *i* the has a higher score in the second text. If the contribution is negative, then its score is higher in the first text.

```{r, fig.align='center', fig.height=6, fig.width=7}
shannon_entropy_shift <- entropy_shift(type2freq_1 = reagan, 
                                       type2freq_2 = bush, 
                                       base = 2)
get_shift_graphs(shannon_entropy_shift, text_names = c("Reagan", "Bush"))
```
We now see other words differentiating Reagan's and Bush's inaugural speeches. Above the cumulative contribution plot are the entropies listed of the inaugural speeches. These entropies are also plotted relative to one another at the top of the plot, and the bar $\Sigma$ shows the direction of their difference. By the entropies and the $\Sigma$, we see that Reagan's speeches are slightly more unpredictable. By the cumulative contribution plot, we see that the top 50 words explain less than 13% of the total difference in entropy between the two texts. 

The contributions $\delta H_i$ are available in the type2shift_score column in the shift_scores data.frame in the shift object. The surprisals are available in the type2score_1 and type2score_2 columns.

##Tsallis Entropy Shifts
The Tsallis entropy is a generalization of the Shannon entropy which allows us to emphasize common or less common words by altering an order parameter $\alpha$ > 0. When $\alpha$ < 1, uncommon words are weighted more heavily, and when $\alpha$ > 1, common words are weighted more heavily. In the case where $\alpha$ = 1, the Tsallis entropy is equivalent to the Shannon entropy, which equally weights common and uncommon words.

The contribution $\delta H_i^{\alpha}$ of a word to the difference in Tsallis entropy of two texts is given by

$$\delta H_i^{\alpha} = \frac{-\bigl(p_i^{(2)}\bigr)^\alpha + \bigl(p_i^{(1)}\bigr)^\alpha}{\alpha - 1}$$.

The Tsallis entropy can be calculated using `entropy_shift` by passing it the parameter `alpha`.

```{r, fig.align='center', fig.height=6, fig.width=7}
tsallis_entropy_shift <- entropy_shift(type2freq_1 = reagan, 
                                       type2freq_2 = bush, 
                                       base = 2,
                                       alpha = 0.8)
get_shift_graphs(tsallis_entropy_shift, text_names = c("Reagan", "Bush"))
```

